{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cf9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72e0e0f972342d8abc2bab331025e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for Masked Language Modeling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e6bcebebba4de184532d5b9da764d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ShadowFox_Projects\\aDVANCED\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Unknown\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970ea4917af643cca13306397cc0cd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ced3974a56460783a1dd6702e02db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3026a92efa7644238f78f251cee28083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ShadowFox_Projects\\aDVANCED\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1f303ac3d847ed8b9183b2213cb352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM Model and helper function ready.\n",
      "\n",
      "--- Running Exploration and Analysis ---\n",
      "Text: The capital of France is [MASK].\n",
      "Predictions: ['paris', 'lille', 'lyon', 'marseille', 'tours']\n",
      "\n",
      "Text: The doctor prescribed the [MASK] to the patient.\n",
      "Predictions: ['drug', 'procedure', 'treatment', 'medication', 'surgery']\n",
      "\n",
      "Text: The mechanic checked the [MASK] of the car.\n",
      "Predictions: ['interior', 'back', 'front', 'side', 'rest']\n",
      "\n",
      "Text: All the players on the team celebrated after [MASK] won the championship.\n",
      "Predictions: ['they', 'having', 'it', 'he', 'we']\n",
      "\n",
      "--- Analysis Complete ---\n",
      "\n",
      "--- Setting up Visualization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ShadowFox_Projects\\aDVANCED\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization ready for text: 'He sat on the river bank.'\n",
      "Run the 'head_view' function below in a Jupyter Notebook to see the visualization.\n"
     ]
    }
   ],
   "source": [
    "# --- Installation (run these first if needed) ---\n",
    "# !pip install torch transformers\n",
    "# !pip install bertviz\n",
    "\n",
    "# --- Imports ---\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from bertviz import head_view\n",
    "\n",
    "# --- Step 2: Load Model and Helper Function ---\n",
    "\n",
    "# Load the pre-trained model and tokenizer for Masked LM\n",
    "print(\"Loading models for Masked Language Modeling...\")\n",
    "tokenizer_mlm = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model_mlm.eval() # Set the model to evaluation mode\n",
    "\n",
    "def predict_masked_word(text, top_k=5):\n",
    "    \"\"\"\n",
    "    Given a text with a [MASK] token, predicts the top_k most likely words \n",
    "    to fill the mask.\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer_mlm(text, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer_mlm.mask_token_id)[1]\n",
    "\n",
    "    # Get model outputs (logits)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_mlm(**inputs)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Get the top_k predictions for the masked token\n",
    "    masked_token_logits = predictions[0, mask_token_index.item()]\n",
    "    top_k_indices = torch.topk(masked_token_logits, top_k, dim=0).indices\n",
    "    top_k_tokens = tokenizer_mlm.convert_ids_to_tokens(top_k_indices)\n",
    "    \n",
    "    return top_k_tokens\n",
    "\n",
    "print(\"MLM Model and helper function ready.\\n\")\n",
    "\n",
    "# --- Step 3: Exploration and Analysis ---\n",
    "\n",
    "print(\"--- Running Exploration and Analysis ---\")\n",
    "\n",
    "# Test Case 1: Basic Factual Knowledge\n",
    "text_1 = \"The capital of France is [MASK].\"\n",
    "predictions_1 = predict_masked_word(text_1)\n",
    "print(f\"Text: {text_1}\")\n",
    "print(f\"Predictions: {predictions_1}\\n\")\n",
    "\n",
    "# Test Case 2a: Semantic Context (Medical)\n",
    "text_2a = \"The doctor prescribed the [MASK] to the patient.\"\n",
    "predictions_2a = predict_masked_word(text_2a)\n",
    "print(f\"Text: {text_2a}\")\n",
    "print(f\"Predictions: {predictions_2a}\\n\")\n",
    "\n",
    "# Test Case 2b: Semantic Context (Automotive)\n",
    "text_2b = \"The mechanic checked the [MASK] of the car.\"\n",
    "predictions_2b = predict_masked_word(text_2b)\n",
    "print(f\"Text: {text_2b}\")\n",
    "print(f\"Predictions: {predictions_2b}\\n\")\n",
    "\n",
    "# Test Case 3: Syntactic and Long-Range Context\n",
    "text_3 = \"All the players on the team celebrated after [MASK] won the championship.\"\n",
    "predictions_3 = predict_masked_word(text_3)\n",
    "print(f\"Text: {text_3}\")\n",
    "print(f\"Predictions: {predictions_3}\\n\")\n",
    "\n",
    "print(\"--- Analysis Complete ---\\n\")\n",
    "\n",
    "# --- Step 5: Visualization of Results ---\n",
    "\n",
    "print(\"--- Setting up Visualization ---\")\n",
    "# We need BertModel (not ForMaskedLM) to get attention outputs\n",
    "model_viz = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "tokenizer_viz = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Process input\n",
    "text_viz = \"He sat on the river bank.\"\n",
    "inputs_viz = tokenizer_viz(text_viz, return_tensors='pt')\n",
    "outputs_viz = model_viz(**inputs_viz)\n",
    "attention = outputs_viz.attentions  # This is a tuple of 12 (layers) tensors\n",
    "\n",
    "print(f\"Visualization ready for text: '{text_viz}'\")\n",
    "print(\"Run the 'head_view' function below in a Jupyter Notebook to see the visualization.\")\n",
    "\n",
    "# Display visualization\n",
    "# This command must be run in a Jupyter Notebook cell to render the interactive UI.\n",
    "head_view(attention, tokenizer_viz.convert_ids_to_tokens(inputs_viz['input_ids'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
